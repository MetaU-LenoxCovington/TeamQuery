{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc8a98ce-486e-4f32-81c2-1714dcdb6375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import base64\n",
    "import re\n",
    "import textwrap\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    PdfPipelineOptions,\n",
    "    RapidOcrOptions,\n",
    "    smolvlm_picture_description,\n",
    ")\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import HTML, display\n",
    "from ollama import chat\n",
    "from PIL import Image\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f329bc95-27dd-4ffa-bef4-8e511ed3384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_options = PdfPipelineOptions(\n",
    "    generate_page_images=True,\n",
    "    images_scale=1.00,\n",
    "    do_ocr=True,\n",
    "    do_picture_description=True,\n",
    "    ocr_options=RapidOcrOptions(),\n",
    "    picture_description_options=smolvlm_picture_description,\n",
    ")\n",
    "\n",
    "converter = DocumentConverter(\n",
    "    format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "428f6304-805a-447c-afc3-2deb73e96f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('test_data/House Rules.pdf')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#document_path = Path(\"test_data/01. House Rules - Current Version.pdf\")\n",
    "document_path = Path(\"test_data/House Rules.pdf\")\n",
    "document_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "672ee4e9-9fa6-461e-b60b-17409487928b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.1 s, sys: 3.27 s, total: 56.4 s\n",
      "Wall time: 23.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "result = converter.convert(document_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "78a81898-663e-41d8-978b-b2916c59b533",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = result.document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ab937-f15a-4973-a6af-d7b4bae30019",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(document.export_to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b88f102a-a59c-4f01-9ea8-33f3d9d047bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_ref='#/pictures/0' parent=RefItem(cref='#/body') children=[RefItem(cref='#/texts/2')] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.PICTURE: 'picture'> prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=110.65599822998047, t=489.8112487792969, r=494.0281982421875, b=200.79742431640625, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 0))] captions=[] references=[] footnotes=[] image=None annotations=[DescriptionAnnotation(kind='description', text='In this image we can see a building with windows and balconies. We can also see a tree and a bicycle. We can also see the sky with clouds.', provenance='HuggingFaceTB/SmolVLM-256M-Instruct')]\n"
     ]
    }
   ],
   "source": [
    "#print(document.pictures[0].annotations)\n",
    "print(document.pictures[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "77dd3aa2-87de-4445-9119-3304296860be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pictures: 1\n",
      "Picture 0: loaded image: False\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of pictures:\", len(document.pictures))\n",
    "for i, pic in enumerate(document.pictures):\n",
    "    print(f\"Picture {i}: loaded image: {pic.image is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c6639ab2-c829-4466-b0aa-e0fed6d71137",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = []\n",
    "for picture in document.pictures:\n",
    "    for annotation in picture.annotations:\n",
    "            annotations.append(annotation.text)\n",
    "assert len(annotations) == len(document.pictures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d2047e7d-9150-4f90-b823-9d656872a1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_occurences(text, target, replacements):\n",
    "    for replacement in replacements:\n",
    "        if target in text:\n",
    "            text = text.replace(target, replacement, 1)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"No more occurences of '{target}' found in the text for replacement ({replacement}).\"\n",
    "            )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "60c0da16-987a-4105-b694-2a4ef2e1bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PLACEHOLDER = \"<!-- image_placeholder -->\"\n",
    "PAGE_BREAK_PLACEHOLDER = \"<!-- page_break -->\"\n",
    "text = document.export_to_markdown(\n",
    "    page_break_placeholder=PAGE_BREAK_PLACEHOLDER, image_placeholder=IMAGE_PLACEHOLDER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7c0a6b-5f7f-4963-9d69-75b03424dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b62abe-0843-4c1b-950f-f19bcc383875",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(replace_occurences(text, IMAGE_PLACEHOLDER, annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c0d2d4dc-3645-4e6c-887e-14c649e1e0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(\n",
    "    document_path: Path, converter: DocumentConverter, n_pages: int = -1\n",
    ") -> str:\n",
    "    result = converter.convert(document_path)\n",
    "    document = result.document\n",
    "\n",
    "    annotations = []\n",
    "    for picture in document.pictures:\n",
    "        print(picture)\n",
    "        for annotation in picture.annotations:\n",
    "            annotations.append(annotation.text)\n",
    "\n",
    "    if(len(annotations) == len(document.pictures)):\n",
    "        print(\"mismatch in number of annotations and number or pictures\")\n",
    "    text = document.export_to_markdown(\n",
    "        page_break_placeholder=PAGE_BREAK_PLACEHOLDER,\n",
    "        image_placeholder=IMAGE_PLACEHOLDER,\n",
    "    )\n",
    "    text = replace_occurences(text, IMAGE_PLACEHOLDER, annotations)\n",
    "    if n_pages == -1:\n",
    "        return text\n",
    "    return PAGE_BREAK_PLACEHOLDER.join(text.split(PAGE_BREAK_PLACEHOLDER)[:n_pages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d999229f-0e27-4253-a079-980e21ce88e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_ref='#/pictures/0' parent=RefItem(cref='#/body') children=[RefItem(cref='#/texts/2')] content_layer=<ContentLayer.BODY: 'body'> label=<DocItemLabel.PICTURE: 'picture'> prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=110.65599822998047, t=489.8112487792969, r=494.0281982421875, b=200.79742431640625, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 0))] captions=[] references=[] footnotes=[] image=None annotations=[DescriptionAnnotation(kind='description', text='In this image we can see a building with windows and balconies. We can also see a tree and a bicycle. We can also see the sky with clouds.', provenance='HuggingFaceTB/SmolVLM-256M-Instruct')]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "document_path = Path(\"test_data/House Rules.pdf\")\n",
    "document_text = process_document(document_path, converter, n_pages=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6031c37e-d4fa-4f43-9346-ece0b40d1e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1615\n"
     ]
    }
   ],
   "source": [
    "print(len(document_text.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9662f710-0197-461e-aa9c-8a2dc79ae5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(document_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8e8fb60f-9299-4c19-8eaa-bcbec1b6f963",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_PATTERN = \"\\n\"\n",
    "chunks = document_text.split(SPLIT_PATTERN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "16d15e52-0dec-494b-8b92-aed509bdd965",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_text = \"\"\n",
    "for i, chunk in enumerate(chunks):\n",
    "    if chunk.startswith(\"#\"):\n",
    "        chunk = f\"#{chunk}\"\n",
    "    chunked_text += f\"<|start_chunk_{i}>\\n{chunk}<|end_chunk_{i}|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6865b143-f2de-412c-9e72-aeade888e31b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(chunked_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6d41a9de-2b3e-4735-af56-4971407fbca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL = \"hf.co/google/gemma-3-12b-it-qat-q4_0-gguf:latest\"\n",
    "MODEL = \"llama3:8b\"\n",
    "TEMPERATURE = 0.0\n",
    "MIN_P = 0.0\n",
    "REPEAT_PENALTY = 1.0\n",
    "TOP_K = 64\n",
    "TOP_P = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3b53dfa6-ed2e-438b-aa2e-bd05d493be70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(prompt: str) -> str:\n",
    "    response = chat(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        keep_alive=\"1h\",\n",
    "        options={\n",
    "            \"num_ctx\": 16384,\n",
    "            \"temperature\": TEMPERATURE,\n",
    "            \"min_p\": MIN_P,\n",
    "            \"repeat_penalty\": REPEAT_PENALTY,\n",
    "            \"top_k\": TOP_K,\n",
    "            \"top_p\": TOP_P,\n",
    "        },\n",
    "    )\n",
    "    return response.message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9c6e2e1a-6602-45dd-bbaa-19a4d4a38f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKING_PROMPT = \"\"\"\n",
    "You are an assistant specialized in splitting text into semantically consistent sections.\n",
    "\n",
    "<instructions>\n",
    "    <instruction>The text has been divided into chunks, each marked with <|start_chunk_X|> and <|end_chunk_X|> tags, where X is the chunk number</instruction>\n",
    "    <instruction>Identify points where splits should occur, such that consecutive chunks of similar themese stay together</instruction>\n",
    "    <instruction>Each chunk must be between 200 and 1000 words</instruction>\n",
    "    <instruction>If chunks 1 and 2 belong together but chunk 3 starts a new topic, suggest a split after chunk 2</instruction>\n",
    "    <instruction>The chunks must be listed in ascending order</instruction>\n",
    "    <instruction>Provide your response in the form: 'split_after: 3, 5'</instruction>\n",
    "</instructions>\"\n",
    "\n",
    "This is the document text:\n",
    "<document>\n",
    "{document_text}\n",
    "</document>\n",
    "\n",
    "Respond only with the IDs of the chunks where you believe a split should occur.\n",
    "YOU MUST RESPOND WITH AT LEAST ONE SPLIT\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76b9cd7-933d-4aec-8676-05599734a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = CHUNKING_PROMPT.format(document_text=chunked_text)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1912b769-b392-4a91-aec5-a1a5ef2e091b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.59 ms, sys: 9.32 ms, total: 13.9 ms\n",
      "Wall time: 39.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = call_model(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b5c79eff-d86a-4958-bb3b-da58fb1b9a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the text, I suggest the following splits:\n",
      "\n",
      "split_after: 2, 6, 10, 14, 18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58, 62, 66, 70, 74, 78, 82, 86, 90, 94, 98, 102, 106, 110, 114, 118, 122, 126\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "27a3953d-8aeb-44be-a405-fa6699260f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_by_llm_suggestions(chunked_text, llm_response):\n",
    "    split_after = []\n",
    "    if \"split_after:\" in llm_response:\n",
    "        split_points = llm_response.split(\"split_after:\")[1].strip()\n",
    "        split_after = [int(x.strip()) for x in split_points.split(\",\")]\n",
    "\n",
    "        print(\"split after:\", split_after)\n",
    "\n",
    "        #return whole text as one chunk if no splits were suggested\n",
    "        if not split_after:\n",
    "            print(\"returned whole chunk\")\n",
    "            return [chunked_text]\n",
    "\n",
    "        chunk_pattern = r\"<\\|start_chunk_(\\d+)\\|?>(.*?)<\\|end_chunk_\\1\\|>\"\n",
    "        chunks = re.findall(chunk_pattern, chunked_text, re.DOTALL)\n",
    "        print(\"chunks: \", chunks)\n",
    "        sections = []\n",
    "        current_section = []\n",
    "\n",
    "        for chunk_id, chunk_text in chunks:\n",
    "            current_section.append(chunk_text)\n",
    "            if int(chunk_id) in split_after:\n",
    "                sections.append(\"\".join(current_section).strip())\n",
    "                current_section = []\n",
    "\n",
    "        # add the last section if it's not empty\n",
    "        if current_section:\n",
    "            sections.append(\"\".join(current_section).strip())\n",
    "\n",
    "        return sections    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306e6680-30c3-4734-9ae1-99d8b22868ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chunks = split_text_by_llm_suggestions(chunked_text, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "adb608aa-7d77-4d36-be40-dfe64849f9fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(llm_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7ca9113a-2bcc-4f5b-a7da-a0375bae73f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- c)Installationsforelectrical power,telephone,television, radio,airconditioning,awningsor any otherpurpose whichshallbevisible fromorprojectfrom,orprotrude outside thephysical confinesofan apartment t orbe attached in anyway to the exterior of thebuilding are prohibitedwithout theprior approval of theBoard or ManagingAgent.\n",
      "- d)Nameplate,signs,signalsorlettersvisibleoutsidean apartmentshall notbe inscribed,placedorexposed on or atanywindow,door,orparking stall unless approved by the Board or the Managing Agent.\"For Sale\",\"For Rent\" and\"Open House\" signs will be regulated by the Board or theManagingAgent.\n"
     ]
    }
   ],
   "source": [
    "print(llm_chunks[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5fc2491f-bb3a-45ca-baee-9ad1b5f363bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, kenlm, wordninja\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1ce497ef-37df-4022-b65b-d0752a17a20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100   111    0   111    0     0    625      0 --:--:-- --:--:-- --:--:--   627\n"
     ]
    }
   ],
   "source": [
    "!curl -L -o en_5gram.bin \\\n",
    "     https://dl.fbaipublicfiles.com/kenlm/models/news/2020-09-05/5gram_spken.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "137e3190-8cdf-4f14-b27c-11d7e382865b",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot read model 'en_5gram.bin' (util/file.cc:76 in int util::OpenReadOrThrow(const char *) threw ErrnoException because `-1 == (ret = open(name, 0x0000))'. No such file or directory while opening /Users/lenox/Desktop/MetaU/TeamQuery/pythonService/en_5gram.bin)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mkenlm.pyx:139\u001b[39m, in \u001b[36mkenlm.Model.__init__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mRuntimeError\u001b[39m: util/file.cc:76 in int util::OpenReadOrThrow(const char *) threw ErrnoException because `-1 == (ret = open(name, 0x0000))'.\nNo such file or directory while opening /Users/lenox/Desktop/MetaU/TeamQuery/pythonService/en_5gram.bin",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[145]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m LM = \u001b[43mkenlm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43men_5gram.bin\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m          \u001b[38;5;66;03m# download a KenLM 5-gram\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msplit_token\u001b[39m(tok):\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# wordninja returns list; only return if at least two parts\u001b[39;00m\n\u001b[32m      5\u001b[39m     parts = wordninja.split(tok)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mkenlm.pyx:142\u001b[39m, in \u001b[36mkenlm.Model.__init__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mOSError\u001b[39m: Cannot read model 'en_5gram.bin' (util/file.cc:76 in int util::OpenReadOrThrow(const char *) threw ErrnoException because `-1 == (ret = open(name, 0x0000))'. No such file or directory while opening /Users/lenox/Desktop/MetaU/TeamQuery/pythonService/en_5gram.bin)"
     ]
    }
   ],
   "source": [
    "LM = kenlm.Model('en_5gram.bin')          # download a KenLM 5-gram\n",
    "\n",
    "def split_token(tok):\n",
    "    # wordninja returns list; only return if at least two parts\n",
    "    parts = wordninja.split(tok)\n",
    "    return parts if len(parts) > 1 else None\n",
    "\n",
    "def better_perplexity(before, after, thresh=0.15):\n",
    "    return (LM.perplexity(before) - LM.perplexity(after)) / LM.perplexity(before) > thresh\n",
    "\n",
    "def unsmash_sentence(sent):\n",
    "    out = []\n",
    "    for tok in re.finditer(r'\\w+|\\W+', sent):\n",
    "        w = tok.group(0)\n",
    "        if w.isalpha() and len(w) > 8:                       # heuristic length\n",
    "            parts = split_token(w.lower())\n",
    "            if parts:\n",
    "                candidate = ' '.join(parts)\n",
    "                if better_perplexity(sent, sent.replace(w, candidate, 1)):\n",
    "                    w = candidate\n",
    "        out.append(w)\n",
    "    return ''.join(out)\n",
    "\n",
    "def unsmash_text(text):\n",
    "    fixed = [unsmash_sentence(s) for s in sent_tokenize(text)]\n",
    "    return ' '.join(fixed)\n",
    "\n",
    "clean_text = unsmash_text(document_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7aa989-4185-45fa-88a6-7adc52b9339c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TeamQuery Python Environment",
   "language": "python",
   "name": "teamquery-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
